TODO:
- pose detection of arbitrary moonboard beta
- overlay of two moonboard beta and their detected poses (one base video with pose detection, one shadow pose overlaid on top)
  - this comes with a subgoal of being able to manipulate the videos so that the board positions and thus the holds overlay properly, so that the poses will also match properly
- selecting timestamps in the video to sync, since if we're overlaying, different moves might take different times in different betas. so basically allowing to select frame ranges in the base and the overlaid videos to properly sync up the comparisons

- prioritization:
  - allow instagram embed links
  - ipad testing
  - make designs prettier
  - synchronize the pose displays better
  - improve boundary selection process
    - maybe can play around with overlays?
  - improve state persistence/management?
    - so like figuring out going back and forwards and stuff
    - and also going back to video upload screen
    - loading and error case handling
  - store video source URLs, and application state, in local storage, so if you refresh the page,
    you won't lose your spot
  - resive observers? for screen sizing which kinda booms the canvases as well
  - improve code quality

- done:
  - improve perspective transform display performance:
    - switched to glfx implementation
    - PR: https://github.com/alanzhu39/alpha-beta/pull/1


8/20/2024:
In progress:
- fix bugs for continuing playback while one video has ended



8/18/2024
In progress:
- test, finalize, and merge in instagram embedding changes
  - testing some actual beta videos: it works!!!
  - also adding edge case where the videos are in sidecar format (eg. slideshow videos)
  - basically just taking the first video_url that exists
  - also have to sort out a bug with the image aspect ratio being weird (eg. square videos)
  - uhh ok, so I think we want to have the reference video size match the user video size, if it exists
  - or more like, the reference canvas size. we don't really care about the video size, actually,
    because we're just using the canvas for the display
  - uhh ok also yeah, just realized <video> element is default display inline, which is why it wasn't obeying all my block sizing
    - lmfao. with display block it becomes ez
  - uhh ok, so for boundary selection we want to allow the canvas to stretch into the whole video-container div
  - but, for video scrubbing, we actually want the canvas to just be the same size as the video?
  - uhhh ok so canvas has its own coordinates, and that's what we're setting with the width and height attributes on the canvas element
    - glfx canvas is not letting me set the coordinates on that element actually, so we might have to work around that?
  - buuut, we actually can set the css styling sizes, via canvasRef.style.width/height. so that's what we want to use for sizing,
    and lining things up
  - set canvas style/offset width/height to video offset width/height
  - set canvas width and height to video size?
- raspberry pi deployment
  - rebased off of instagram changes
- make designs prettier
  - use rectangle drawing for making box instead of doing point-by-point?
  - makes it easier to line up the edges of the board
  - also more intuitive on ipad I think?
    - tapping multiple times is less comfortable than just dragging the points
- loading, failure, and more state change cases?
  - like, going between pages in the sequences
- ipad testing



8/14/2024:
In progress:
- ok so did some investigation yesterday, and like over the weekend
- I guess instaloader is using the graph endpoint which fetches the video metadata in a specific graphql query
  - see https://github.com/instaloader/instaloader/blob/e159065799ee4a1813185095bc4b663a1073fe3c/instaloader/structures.py#L320
  - this is the method that populates the metadata that is then read for the video_url property
- I was able to find this same video source URL using the search params: /?__a=1&__d=dis
  - but I heard you'll also get rate limited on this
  - so I think the idea will just be, create a method for each method, and then wrap each in a try-catch
    so you can fall back to the next one if the first one is rate limited
  - ordering in order of like reliability
- uhhh ok so it looks like instaloader went back to working for me lol
  - I think it definitely does seem like that graphql /query endpoint is meant to serve these logged-out external requests
  - I think the problem will just be dealing with rate limits if we have to
  - maybe even the Philz was banned lmao?
  - but yeah have some ideas for getting the rate limiting if we have to, namely maybe adding some more fields to the query request
    using the curls that I grabbed.
  - and seeing if we can reverse engineer anything out of the API calls that are done to show the public posts

Done:
- support instagram embed links:



8/8/2024:
In progress:
- support instagram embed links:
  - using separate backend service
  - still haven't found the cheapest hosting option, but render should be an ok free system for testing
  - yeahhh so running into more issues with each of these options we're testing:
    - render blocks outbound requests to instagram.com LOL
    - ploomber is just broken
  - I think honestly the best bet is to just host the whole frickin app on the raspberry pi
  - then we can even just use the API route. big
  - uhhhh we might have a problem getting it to serve from a static IP though?
    - hmm can look into using duckdns for free dynamic dns
    - I think we just set up a cron job to register the current IP daily to our chosen hostname?
    - or some other free dns service
    - poggg
  - nahh so basically just tunneling this via cloudflare



8/5/2024:
In progress:
- support instagram embed links:
  - trying to add pyodide to run the script in a web worker
  - unfortunately, it seems actually like it doesn't let us send the XML requests to the instagram API?
  - due to cors errors
  - and I don't think there's any way to disable this from the app?
    - since it's a browser protection
  - ok new solution:
    - create full-fledged new API flask service to serve the post loader endpoint
    - could maybe even convert this to an actual app haha



7/30/2024:
In progress:
- support instagram embed links
  - create API route to call instaloader python library and return video source URL
  - then use this video source URL as the source inside the video element
  - so yeah tested out py2wasm compiler, didn't really work great (unless I used wasmer runtime)
    - and even then it like couldn't take the argument that I wanted lol
  - so thinking about just doing the API route version and using node child_process?
    - will need to do sanitization on the video URL input.
    - I think on client will just strip the query params, and on server will make sure it's only:
      - [a-zA-Z:/\-] characters, shouldn't need query params for the actual post URLs
  - err ok so we don't have python in the netlify function X fucking D
  - fucking aws lambda wrapper companies, so shit
  - like literally why tf do you exist
  - also tried compiling my python to JS but no tools that can automatically do the library I think
  - plan B is embedding pyodide on browser and just straight executing script on the client
  - guess we go for this
  - so, run pyodide in a web worker on page load, and also install instaloader
  - then we execute the script that we want (very simple) via the web worker as well
- test on ipad



7/27/2024:
In progress:
- support instagram embed links
  - create API route to call instaloader python library and return video source URL
  - then use this video source URL as the source inside the video element
- test on ipad

notes:
- learned about some GCC linker flags I guess?
- also learned about python to wasm compilation
  - lowkey basically hard/impossible to *translate* python (dynamically typed language) to C?
  - interpreting it is easy I guess? cuz you can simulate its runtime model easily with a language like C?
  - so the best ideas for porting python to other languages is to port the entire CPython library itself lmao
  - or at least specifically like for the web (WASM, JS), you just transpile CPython to wasm since C is easier to convert to wasm which is basically a kind of assembly lanuage
  - and so py2wasm is python -> C (nuitka) -> wasm (emscripten)?
  - and I guess python -> C is the harder part
  - but also it was trying to get me to link the generated C program somehow that was breaking
- wasm-ld is a wasm-specific linker
- getting exposed to some wasm ideas? but definitely a lot more to learn
  - also it's not the same as asm.js
- gcc linker args are comma separated after -Wl
- rpath is like the path you look for library files (I guess like stdlib and other stuff) to link w your C program
- $ORIGIN is like linux specific and points to the origin of your compilation or something?
- tried out claude, the responses seem slightly better than chatgpt, but lowkey UI is worse?
  - I might be a gemini glazer
  - use LLMs more for questions actually!
- --disable-new-dtags: something about dynamic tags in ELF binaries?
  - I remember seeing something about ELF binaries in tsoding video, just like a common binary format for linux platforms?

Done:
- uh fuck and also debugging the first frame drawing on the normal canvas??
  - what's working for me is just seeking to currenttime = 0 lmao which triggers the onSeek frame draw. nice!
  - not working for user pose
  - uhh so I guess maybe looking at the video ref and why it's not showing up?
  - uh ok so when stepping, it seems the video is drawn on the frame...
  - but maybe sometimes the video ref is still null?
  - yeah idk wtf the actual cause is lol
    - but seems other people have similar issue
- fix perspective shift with glfx
  - yeah so just relying on the perspective transform from glfx instead of doing it ourselves lmao
  - there's a slight offset on the transformed video?
  - and also just like figuring out the dimensions of the frame canvas in glfx
    - mm yeah doing this actually not great, since it actually seems to affect the resolution of the output!
    - so maybe trying out now just using the user and reference poses directly
  - and I think maybe can try out just inputting the user and reference pose stores into the perspective directly
    - poggg this works really well actually
  - then we wouldn't even have to calculate the transform! yes!
  - I'm pretty sure the transform itself is right, but maybe retest it
    - yeah retested and it still looks fine, so probably not problem with that
- improve perspective transform performance:
  - smooth out glfx bugs
  - updated the pose updates logic to be cleaner and just redraw the landmarks themselves (instead of the whole frame)
  - separating frame and display canvases for user frame as well!
    - yeah so drawing video frame to frame-canvas in user video



7/18/2024:
In progress:
- improve perspective transform performance
  - considering using different libraries for this (not perspectivets since it's hella slow)
  - experimenting with glfx-es6
  - I guess we have to append the display canvas directly if we're rendering the transformed view
  - uhhh I guess it's kinda working with glfx? may have to smooth some things out in the rendering pipeline still
    - like pause/play sometimes doesn't work
    - and also the perspective shift is like very slightly off for some reason?
    - oh and also you have to use the video dimensions instead of canvas dimensions because the webgl context has those dimensions
    - maybe that's screwing with it?
    - check the canvas dimensions on initialization
    - or can we set video width and height on initialization?
    - ohh and also I think sometimes the video ref is not initialized when we try to initialize the canvas?
      - cuz there's like this texture undefined error thingy sometimes
- allow instagram embed links
  - so I think we can use this python library, make an api endpoint to get video URL from post URL, then use that as video source
- test on ipad

notes:
- ok so it seems like performance bottleneck is basically drawing all the slices of the original image?
- so I think just drawing on the display canvas won't really help cuz that's just the last step
- can we maybe figure out if we can use the actual canvas perspective shift API?



7/13/2024:
In progress:
- improve perspective transform display performance
- clean up pose synchronization code? could maybe refactor some of the logic

Done:
- synchronize pose displays better
  - so like when you play eg. the reference video, the pose should update on the user video
  - should we just bind the canvases between the components?
  - but the two flows actually don't share any context either
  - uh ok so I made it so that like, if you're drawing the user canvas, and you detect a change in the reference pose store, then you also draw that one
  - seems to make the performance slower though, like 20fps -> 10fps
  - will do some testing on the cause
    - ah ok so yeah, it's the same thing of the perspective drawing being the bottleneck I think
    - like, when we draw the user pose onto the reference canvas, we still have to redraw the underlying image, which is perspective shifted
    - so that calls the perspective API which is slow
    - notice that on the other side, adding the drawing on the user canvas from the reference pose updates introduces no significant performance decrease
    - performance is still around 10 fps which is the same as just drawing the reference canvas itself
  - so yeah looks like next step would be trying to improve perspective performance

- tidy up designs
- fix perspective drawing
  - can we also maybe improve the performance?
  - right now it like copies stuff to a canvas...
  - can we tweak the internals to just draw straight to the display canvas?
- improvements:
  - make detected poses be drawn on both canvases...
    - like right now it only draws on the canvas that you're playing on
  - boundary selection UX, since sometimes holds can still be like slightly misaligned...
    - how can we let the user adjust this as they're scrubbing?
    - or maybe we can adjust one screen while the other one is already playing?
    - also maybe allow scrubbing on the initial frame corner selection page
      so you can account for slight adjustements during the video and occluded corners
  - displaying first frame on load of the scrubber screen component
  - better scrubber UI
  - better colors and fonts
  - testing on ipad



6/30/2024:
In progress:
- hmm so actually I feel like we can just have the user copy/paste the embed code?
- utilize the searching and stuff from the existing moonboard app lol
- wait lol so the performance doesn't actually change if we detect off of the display canvas vs the video
  - of course lol
  - the detection canvas is just there so we have a webgl2 context that we can run GPU stuff in
  - we're not actually ever drawing anything to the detection canvas
- so the performance is probably bottlenecked on drawing the perspective transform?
  - and specifically I think creating the new PerspectiveTransform object on each frame lol
  - cuz looking at the internals it makes whole new canvas nodes
- ok yeah so once we remove the perspective shift step, it's back to 20fps
  - and we won't even need the apply transform to pose
  - lose about 1fps adding that actually even lol
- uhhh shit looks like we have to reinstantiate perspective transform for each frame??
  - ohh I think that makes sense? since it's assuming it's reading from an image
  - maybe we can patch to accept video
- hmm even with porting over perspective stuff, it seems to be going a little slow per frame...
  - I think on the draw there's still some dom node creation which we could speed up?
  - is this doing some thing for each offset?



6/28/2024:
In progress:
- just doing reference pose detection on the transformed canvas
  - hmmm this is actually a bit slower though?
  - oh I think that makes sense as we're no longer doing detection on the detection canvas
  - so maybe draw onto the detection canvas as well for display
  - ohh shit but we can't draw in perspective to the detection canvas
  - so this is why we have to apply transform to the landmark instead?



6/21/2024:
In progress:
- moonboard api investigation:
    POST https://www.moonboard.com/Video/GetVideos
    form data: sort=&page=1&pageSize=8&group=&filter=Grade~eq~'6B%2B'
  - not sure if we need auth on this?
- fix pose perspective
  - make separate canvas to have perspective-shifted pose?
  - orrr, could we even just calculate the pose transform ourself and then just draw that?
  - and skip transforming the canvas



6/19/2024:
In progress:
- TODO: add pose detection (mediapipe vision npm package. has TS support! thank god)
- do searching from moonboard video database

some performance investigations:
- hooked up mediapipe and it works pretty much first try!
- FPS with just copying image onto canvas is hella fast: 60fps which is basically just what the video was taken at
- FPS with pose detection drops to about 10fps
- can GPU make it faster?
- alright so switched to GPU delegation to a detection canvas (it looks like it just needs the WebGL2 rendering context to activate the GPU or something?)
- but yeah then just rendered the detected landmark using the normal 2d context for drawing utils and it still worked
- and best part is we got free 2x fps speedup! went up to 20fps



6/16/2024:
In progress:

Done:
- making video scrubber work
  - adding scrubber range input
  - adding event listeners to render frames
- updating boundary selection flow
- port over transform calculation
- add perspectivets for rendering perspective transformations on canvas
- doing a shit ton of patches to make that shit work



6/15/2024:
In progress:
- starting svelte components
- Layout
  - 

Svelte notes:
- the unit for reactivity is variables
- the catalyst for reactivity is reassignment of state variables
- $ reactive statements are kinda like effects?
  - they get ran if any of the variables they depend on are reassigned
  - basically like useEffect and its dependencies
- stores: like context in React
  - store values can be referenced by prefixing their variables with $ actually!
  - can directly read/assign to these $variables



6/13/2024:
In progress:
- new Svelte app?
- start doing state variables and stuff
- select points flow
- canvas perspective



6/11/2024:
In progress:
- select points flow
- can we get the video processing to be more performant?
  - multithreading?
  - GPU?
  - can we lower the onseek time?
- canvas perspective
- maybe start actual app setup. will probably want to transition into an actual framework?
- oh and also maybe try to get different canvas layers?
  - since we want the poses to be on different layers so we can superimpose them in the different panels



6/9/2024:
In progress:
- pursue sort of preprocessing step to aid in both performance and video scrubbing:
  - so the idea is, we load the video at like 30 fps into a frame buffer within a canvas context
  - during this loading, we also draw the pose detection for each frame
  - then, after processing, we use a custom video-scrubber component to like detect which frame to display on the fly
    - and that should be much faster than doing it live

Done:
- tested preprocessing and scrubbing with good results.
  - this may be the way we want to go, but it does take a while to preprocess



6/8/2024:
In progress:
- test video upload and pose detection flow
  - we want to precompute all the poses for the video?
  - play video and run detect video on each frame
- webapp version
  - video upload flow
  - select video flow
  - select points flow
  - mediapipe detection
  - drawing videos and pose references using canvas

Done:
- video upload and manipulation
- prelim testing w JS mediapipe library
  - prob want to use heavy model for best accuracy



6/6/2024:
In progress:
- start web app version:
  - start with mediapipe web impl tutorial
  - ipad width, landscape
  - side-by-side video playback
    - video.js playback
  - upload your beta video, select moonboard points
  - select reference beta video, select moonboard points
  - run ffmpeg on reference beta video to transform it to fit the same frame
    - ffmpeg.wasm?
    - maybe too slow
    - html5 video + canvas
  - run pose detection on both videos
    - mediapipe
  - overlay poses (using canvas?)
    - mediapipe



6/4/2024:
In progress:
ffmpeg filters:
- adding border box: pad
- cutting video: -ss
- perspective of course



5/28/2024:
In progress:
- ffmpeg filters
- mediapipe web implementation? can leverage OpenGL ES on Android/Linux, and Metal on iOS?
- video side-by-side mode
  - using scrubbers for each video to sync beta videos
- video texture mapping to align boards
- overlay poses between videos

- we know mapping of source -> dest moonboard corner points
- then, using that linear interpolation, calculate the coordinates of the video corner points
- finally, apply ffmpeg perspective filter

sample flow:
1. select a problem, choose a reference beta video
  a. can we store on some server the video and pose detected positions? and then do overlay with the pose jsons?
  b. should also already have alignment points defined
2. upload user beta video
3. select video alignment points
4. align videos
  a. might want to do this server-side with ffmpeg
5. do pose detection on user beta video
6. display videos with overlays



5/22/2024:
In progress:
- mediapipe implementation with some sample videos
- testing different CV libraries for rock climbing poses
- openpose, openpifpaf, mediapipe
	- two examples of previous work that I've seen seem to use mediapipe

Done:
- tested a couple different implementations using the mediapipe python API
- actually the legacy solutions API seemed to perform the best, so that was weird?
- lowkey switching up the color scheme to kinda help I think increase contrast between the person and the moonboard background might be something helpful?
- there's still a lot of stuttering kinda stuff and flickering that makes it look kinda wonky sometimes, and the model also doesn't deal with the occlusion and hidden limbs well
  - In the problem I tested there's often moments where an arm or leg is occluded cuz of like drop knees or just twisting hips/whatever, and the limb detection kinda freaks out in those moments
- maybe want to see if a different model can do a better job handling that
- also maybe want to see what the deal with the legacy solutions API is? is that an old model or what?
- also discovered that the video mode is indeed better for videos lmao
  - I didn't realize but I guess it makes sense
  - my intuition at first was that the image mode might spend more time on each frame, while the video mode was doing some type of interpolation/trying to go faster to increase throughput
  - but I think the video mode throughput is the same, and it just has the added benefit of seeeing the before and after frames and that helps smoothen out the posts throughout movements
  - just doing images frame-by-frame makes it suuuper stuttery and jerky as they don't have context of the overall movement over time
- overall pretty happy with progress today, but I think we still need to look for some improvements on the model detection

Mediapipe:
- this shit looks hella pop, and is supported by lots of google research
- have web solution which I'm thinking of trying out (although maybe python for easier prototyping?)

OpenPose notes:
- uhh alright so spent like two hours trying to set up all these nvidia drivers and cuda shit, and it seems to be kinda working? but apparently mediapipe kinda just pops openpose?
